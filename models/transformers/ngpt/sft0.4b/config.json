{
  "architectures": [
    "NGPT"
  ],
  "auto_map": {
    "AutoConfig": "config_ngpt.NGPTConfig",
    "AutoModelForCausalLM": "model_ngpt.NGPT"
  },
  "dim": 1024,
  "dropout": 0,
  "max_position_embeddings": 1024,
  "model_type": "ngpt",
  "n_blocks": 20,
  "n_heads": 16,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "vocab_size": 32584
}
